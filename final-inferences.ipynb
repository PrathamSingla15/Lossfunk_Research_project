{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q torch transformers peft datasets tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-01T22:25:44.739886Z","iopub.execute_input":"2025-06-01T22:25:44.740602Z","iopub.status.idle":"2025-06-01T22:27:25.204225Z","shell.execute_reply.started":"2025-06-01T22:25:44.740573Z","shell.execute_reply":"2025-06-01T22:27:25.203094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nfrom datasets import load_dataset\nimport json\nfrom tqdm import tqdm\n\nnum_samples = None  \noutput_file = \"hard_label_test.json\"\nsave_batch_size = 50  \n\n# Load the PEFT model with device_map=\"auto\" for multi-GPU support\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"unsloth/Qwen3-4B-Base\", \n    torch_dtype=torch.float16,\n    device_map=\"auto\"  \n)\n\npeft_model_id = \"yobro4619/hard_labels_final\"\nmodel = PeftModel.from_pretrained(base_model, peft_model_id)\nprint(model) \n\ntokenizer = AutoTokenizer.from_pretrained(\"unsloth/Qwen3-4B-Base\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nif not hasattr(tokenizer, 'chat_template') or tokenizer.chat_template is None:\n    print(\"No chat template found, setting a default one for Qwen...\")\n    tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'system' %}<|im_start|>system\\n{{ message['content'] }}<|im_end|>\\n{% elif message['role'] == 'user' %}<|im_start|>user\\n{{ message['content'] }}<|im_end|>\\n<|im_start|>assistant\\n{% endif %}{% endfor %}\"\n\ndataset_name = \"yobro4619/math-reasoning-dataset\"\ntest_dataset = load_dataset(dataset_name, split=\"test\")\nprint(f\"Loaded test dataset with {len(test_dataset)} samples\")\n\nprint(f\"Dataset columns: {test_dataset.column_names}\")\n\nif num_samples is not None:\n    test_dataset = test_dataset.select(range(min(num_samples, len(test_dataset))))\n    print(f\"Processing {len(test_dataset)} samples as requested\")\n\nsystem_prompt= \\\n\"\"\"You are given a math problem.\n\n1. Carefully analyze the problem.\n2. Show all your working out and reasoning steps very concisely.\n3. Place all reasoning **only once** between the tags <start_working_out> and <end_working_out>.\n4. Then, provide the final answer **only once** between the tags <SOLUTION> and </SOLUTION>.\n5. Do not repeat any part of your response after these tags have been used.\n\nStick strictly to this format.\n\nExample Question-1: An Informal Gathering occurs when a group of people get together in a casual, relaxed manner. Which situation below is the best example of an Informal Gathering?   \nA. After finding out about his salary raise, Jay and a few colleagues go out for a quick dinner after work.  B. Meena sends out 10 invitations for a bachelorette party she is giving for her elder sister.\nAnswer:\n<start_working_out>The question asks me to identify the best example of an \"Informal Gathering,\" defined as a group getting together in a casual, relaxed manner. I need to evaluate which of the two situations provided best fits this definition.\nThe first situation describes a few colleagues going out for a quick dinner after work to celebrate one person's salary raise. This seems spontaneous (\"go out for a quick dinner\") and lacks formal planning. Getting together after work with colleagues can certainly be casual and relaxed, especially when prompted by an immediate event like good news. It aligns well with the core idea of informality.\nThe second situation describes someone sending out invitations for a bachelorette party. The act of sending invitations implies planning and a degree of formality. While the party itself might aim for a relaxed atmosphere, the organization process (invitations, specific event type) makes the gathering itself less \"informal\" in its conception compared to a spontaneous decision to grab dinner. Bachelorette parties often have planned elements and aren't typically characterized by the same level of casual spontaneity as the first scenario.\nComparing the two, the first situation embodies the 'casual' and 'relaxed manner' aspect more strongly due to its apparent spontaneity and lack of formal structure like invitations. The second situation, while a social gathering, involves planning and formal invitations, making it less representative of a purely informal gathering according to the definition provided. So I think that the first scenario is the better example of an informal gathering<end_working_out>\n<SOLUTION>A</SOLUTION>\n\"\"\"\n\ndef generate_response(prompt, model, tokenizer, system_prompt):\n    try:\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": prompt} \n        ]\n        input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    except Exception as e:\n        print(f\"Chat template failed: {e}\")\n        print(\"Falling back to simple concatenation...\")\n        input_text = f\"{system_prompt}\\n\\nUser: {prompt}\\nAssistant:\"\n    \n    inputs = tokenizer(input_text, return_tensors=\"pt\")\n\n    if hasattr(model, 'hf_device_map') and model.hf_device_map:\n        first_device = next(iter(model.hf_device_map.values()))\n        if isinstance(first_device, str) and first_device != \"cpu\":\n            inputs = {k: v.to(first_device) for k, v in inputs.items()}\n    \n    # Generate\n    with torch.no_grad():\n        output_ids = model.generate(\n            **inputs,\n            max_new_tokens=1024, \n            do_sample=True,\n            temperature=0.7,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    generated_ids = output_ids[0][inputs.input_ids.shape[1]:]\n    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n    \n    return response\n\nresults = []\n\nfor i, example in enumerate(tqdm(test_dataset)):\n    prompt = example[\"prompt\"]  \n    solution = example[\"solution\"]  \n    \n    response = generate_response(prompt, model, tokenizer, system_prompt)\n    \n    result = {\n        \"id\": i,\n        \"prompt\": prompt,\n        \"solution\": solution,\n        \"response\": response\n    }\n    \n    results.append(result)\n    \n    if (i + 1) % save_batch_size == 0:\n        with open(f\"interim_results_{i+1}.json\", \"w\") as f:\n            json.dump(results, f, indent=2)\n        print(f\"Saved interim results for {i+1} samples\")\n\n# Save all responses to a JSON file\nwith open(output_file, \"w\") as f:\n    json.dump(results, f, indent=2, ensure_ascii=False)\n\nprint(f\"Inference completed. Results saved to {output_file}\")\nprint(f\"Total samples processed: {len(results)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T22:27:25.924814Z","iopub.execute_input":"2025-06-01T22:27:25.925100Z","iopub.status.idle":"2025-06-01T22:35:18.485520Z","shell.execute_reply.started":"2025-06-01T22:27:25.925077Z","shell.execute_reply":"2025-06-01T22:35:18.484611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ninput_file = \"/kaggle/working/hard_label_test.json\" \noutput_file = \"/kaggle/working/hard_label_test_extracted.json\"  \n\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(\"unsloth/Qwen3-4B-Base\")\nif tokenizer.eos_token is None:\n    tokenizer.eos_token = \"</s>\"  \n\nprint(f\"Using EOS token: {tokenizer.eos_token}\")\n\nreasoning_end = r\"<end_working_out>\"\nsolution_start = r\"<SOLUTION>\"\nsolution_end_regex = r\"</SOLUTION>[\\s]{0,}\" + \\\n    \"(?:\" + re.escape(tokenizer.eos_token) + \")?\"\n\n# Pattern to match the complete solution tags with content\nmatch_format = re.compile(\n    rf\"{solution_start}(.+?){solution_end_regex}\"\n    rf\"[\\s]{{0,}}\",\n    flags=re.MULTILINE | re.DOTALL\n)\n\n# Pattern to extract fraction answers like \\frac{3}{2} (check this FIRST before numbers)\nmatch_fractions = re.compile(\n    solution_start + r\".*?(\\\\frac\\{[^}]+\\}\\{[^}]+\\})\",\n    flags=re.MULTILINE | re.DOTALL\n)\n\n# Pattern to extract numerical answers (but not if part of a fraction)\nmatch_numbers = re.compile(\n    solution_start + r\"(?!.*\\\\frac).*?[\\s]{0,}([-]?[\\d\\.\\,]{1,})(?!\\})\",\n    flags=re.MULTILINE | re.DOTALL\n)\n\n# Pattern for multiple choice answers (A, B, C, D, etc.) - should be standalone letter\nmatch_choice = re.compile(\n    solution_start + r\".*?[\\s]{0,}([A-Z])(?:\\s|</SOLUTION>)\",\n    flags=re.MULTILINE | re.DOTALL\n)\n\ndef extract_answer(response_text):\n    \"\"\"\n    Extract answer from model response STRICTLY from within <SOLUTION></SOLUTION> tags\n    Priority order:\n    1. Try fraction answer (check first to avoid conflicts with numbers)\n    2. Try numerical answer\n    3. Try multiple choice answer\n    4. Return complete content inside SOLUTION tags\n    \"\"\"\n    \n    solution_matches = match_format.findall(response_text)\n    if not solution_matches:\n        return \"\"  \n    \n    solution_content = solution_matches[0].strip()\n    \n    solution_content = re.sub(re.escape(tokenizer.eos_token), \"\", solution_content).strip()\n    \n    \n    fraction_pattern = re.compile(r'(\\\\frac\\{[^}]+\\}\\{[^}]+\\})')\n    fraction_matches = fraction_pattern.findall(solution_content)\n    if fraction_matches:\n        return fraction_matches[0].strip()\n    \n    number_pattern = re.compile(r'^\\s*([-]?[\\d\\.\\,]+)\\s*$|(?:^|\\s)([-]?[\\d\\.\\,]+)(?:\\s|$)')\n    number_matches = number_pattern.findall(solution_content)\n    if number_matches:\n        for match_tuple in number_matches:\n            for match in match_tuple:\n                if match:\n                    return match.strip()\n    \n    choice_pattern = re.compile(r'(?:^|\\s)([A-Z])(?:\\s|$)')\n    choice_matches = choice_pattern.findall(solution_content)\n    if choice_matches:\n        return choice_matches[0].strip()\n    \n    return solution_content\n\n\n\nprint(f\"\\nLoading input file: {input_file}\")\ntry:\n    with open(input_file, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    print(f\"Loaded {len(data)} samples\")\nexcept FileNotFoundError:\n    print(f\"Error: File {input_file} not found!\")\n    exit(1)\nexcept json.JSONDecodeError as e:\n    print(f\"Error: Invalid JSON in {input_file}: {e}\")\n    exit(1)\n\nprocessed_count = 0\nextraction_stats = {\n    \"numerical\": 0,\n    \"fraction\": 0, \n    \"choice\": 0,\n    \"complete\": 0,\n    \"empty\": 0\n}\n\nfor i, sample in enumerate(data):\n    if \"response\" not in sample:\n        print(f\"Warning: Sample {i} missing 'response' field\")\n        sample[\"extracted_answer\"] = \"\"\n        extraction_stats[\"empty\"] += 1\n        continue\n    \n    extracted_answer = extract_answer(sample[\"response\"])\n    sample[\"extracted_answer\"] = extracted_answer\n    \n    if not extracted_answer:\n        extraction_stats[\"empty\"] += 1\n    elif re.match(r'^[-]?[\\d\\.\\,]+$', extracted_answer):\n        extraction_stats[\"numerical\"] += 1\n    elif extracted_answer.startswith('\\\\frac'):\n        extraction_stats[\"fraction\"] += 1\n    elif re.match(r'^[A-Z]$', extracted_answer):\n        extraction_stats[\"choice\"] += 1\n    else:\n        extraction_stats[\"complete\"] += 1\n    \n    processed_count += 1\n\nprint(f\"Saving results to: {output_file}\")\nwith open(output_file, 'w', encoding='utf-8') as f:\n    json.dump(data, f, indent=2, ensure_ascii=False)\n\nprint(f\"\\nProcessing completed!\")\nprint(f\"Total samples processed: {processed_count}\")\nprint(f\"Extraction statistics:\")\nprint(f\"  - Numerical answers: {extraction_stats['numerical']}\")\nprint(f\"  - Fraction answers: {extraction_stats['fraction']}\")\nprint(f\"  - Multiple choice answers: {extraction_stats['choice']}\")\nprint(f\"  - Complete text answers: {extraction_stats['complete']}\")\nprint(f\"  - Empty/No answer found: {extraction_stats['empty']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T22:43:26.142096Z","iopub.execute_input":"2025-06-01T22:43:26.142771Z","iopub.status.idle":"2025-06-01T22:43:26.839175Z","shell.execute_reply.started":"2025-06-01T22:43:26.142744Z","shell.execute_reply":"2025-06-01T22:43:26.838393Z"}},"outputs":[],"execution_count":null}]}